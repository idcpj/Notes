[TOC]

## urllib包
> 是Python3.x中提供的一系列操作URL的库,可以模拟用户使用浏览器访问网页

### 不带请求头实例
     resp = request.urlopen('http://www.baidu.com')
     print(resp.read().decode('utf-8'))
     
### 带请求头
     req = request.Request('http://www.baidu.com')
     req.add_header("User-Agent","Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/61.0.3163.100 Safari/537.36")
     resp = request.urlopen(req)
     print(resp.read().decode('utf-8'))
     
### Post带参数
    req = Request('http://www.thsrc.com.tw/tw/TimeTable/SearchResult')
    postData = parse.urlencode([
        ('StartStation', '977abb69-413a-4ccf-a109-0272c24fd490'),
        ('EndStation', '2f940836-cedc-41ef-8e28-c2336ac8fe68'),
        ('SearchDate', '2017/10/06'),
        ('SearchTime', '17:00'),
        ('SearchWay', 'DepartureInMandarin')
    ])
    req.add_header('Origin','http://www.thsrc.com.tw')
    req.add_header("User-Agent","Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/61.0.3163.100 Safari/537.36")
    resp = request.urlopen(req,data=postData.encode('utf-8'))
    print(resp.read().decode('utf-8'))
    

## requests-比urllib2简洁
>文档地址:[中文文档](http://docs.python-requests.org/zh_CN/latest/)

```
#GET带参数
payload = {'key1': 'value1', 'key2': 'value2'}
r = requests.get("http://httpbin.org/get", params=payload)

#定制请求头
headers = {'user-agent': 'my-app/0.0.1'}
r = requests.get('http://www.baidu.com', headers=headers)

#post表单形式
dict = {"name":"cpj",'age':"123"}
requests.post("http://www.baidu.com",data=dict)

#异常处理
import requests
from requests import exceptions

try:
    response = requests.get("http://httpbin.org/ip",timeout=0.2)
    response.raise_for_status()  #抛出非200 异常的解释

except exceptions.Timeout as e:
    print("请求超时:",e)
except exceptions.HTTPError as e:
    print("状态码异常",e)
else:
    print(response.text)
    print(response.status_code)

#proxies代理
import requests
proxies = {'http': "socks5://127.0.0.1:1080",'https': "socks5://127.0.0.1:1080"}
response = requests.get('https://www.google.com',proxies=proxies)
print(response.content)
```

## BeautifulSoup-解析html
>文档地址  [中文文档](https://www.crummy.com/software/BeautifulSoup/bs4/doc/index.zh.html)


code
```
html_doc = """
<html><head><title>The Dormouse's story</title></head>
<body>
<p class="title"><b>The Dormouse's story</b></p>
<p class="story">
    Once upon a time there were three little sisters; and their names were
    <a href="http://example.com/elsie" class="sister" id="link1">Elsie</a>,
    <a href="http://example.com/lacie" class="sister" id="link2">Lacie</a> and
    <a href="http://example.com/tillie" class="sister1" id="link3">Tillie</a>;
    and they lived at the bottom of a well.
</p>

<p class="story">...</p>
"""

soup = bs4.BeautifulSoup(html_doc, "html.parser")
print(soup.a) #打印<a class="sister" href="http://example.com/elsie" id="link1">Elsie</a>
print(soup.a.string) #打印a标签的内容
print(soup.a['href']) #打印a标签的href属性的值
print(soup.find(id='link3'))
print(soup.find('a',class_='sister'))  #Python的class 有关键字所以加'_'
print(soup.find_all('a',class_='sister'))
print(soup.find('p',{'class','story'}).get_text())
print(soup.find_all("a",href=re.compile(r'^http://example.com')))
print(soup.find_all("input",type=re.compile('text'))) 
```

## pdfminer3k-解析pdf
```
import logging
from urllib.request import urlopen

logging.Logger.propagate = False
logging.getLogger().setLevel(logging.ERROR)

from pdfminer.converter import PDFPageAggregator
from pdfminer.layout import LAParams
from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter
from pdfminer.pdfparser import PDFParser, PDFDocument

fp = open('template/pdftest.pdf', 'rb')
# 在线
# fp = urlopen('http://www.tencent.com/zh-cn/articles/8003251479983154.pdf')

# 创建一个与文档关联的解析器
parser = PDFParser(fp)

# PDF文档对象
doc = PDFDocument()

# 链接解析器和文档对象
parser.set_document(doc)
doc.set_parser(parser)

# 初始化文档
doc.initialize("")

# 创建DPF资源管理器
resource = PDFResourceManager()

# 参数分析器
laparam = LAParams()

# 聚合器
device = PDFPageAggregator(resource, laparams=laparam)

# 创建页面解析器
interpreter = PDFPageInterpreter(resource, device)

# 使用文档对象从pdf中读取内容
for page in doc.get_pages():
    # 使用页面解析器
    interpreter.process_page(page)

    layout = device.get_result()
    # 使用聚合器获取内容
    for out in layout:
        # 判断是否有get_text属性
        if hasattr(out, 'get_text'):
            print(out.get_text())

if __name__ == '__main__':
    pass
```
## pymysql -python3的mysql库

### 事务回滚
```python
import pymysql.cursors
connection = pymysql.connect(host='localhost', user='root', password='root', db='test', charset='utf8mb4', cursorclass=pymysql.cursors.DictCursor)
try:
    with connection.cursor() as cursor:
        sql = "INSERT INTO `urllist` (`urlname`, `urlhref`) VALUES (%s, %s)"
        cursor.execute(sql, ('webmaster@python.org', 'very-secret'))
        print(cursor.rowcount) #受影响条数
        sql = "UPDATE `urllist` set  urlname=%s,urlhref=%s where id=%s"
        cursor.execute(sql, ('cpjupdate', 'hrefupadte', '13'))
        print(cursor.rowcount)

    connection.commit()
except Exception as  e:
    connection.rollback()
    print("mysq:", e)
finally:
    connection.close()

```
