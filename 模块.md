[TOC]

## urllib包
> 是Python3.x中提供的一系列操作URL的库,可以模拟用户使用浏览器访问网页

### 不带请求头实例
     resp = request.urlopen('http://www.baidu.com')
     print(resp.read().decode('utf-8'))
     
### 带请求头
     req = request.Request('http://www.baidu.com')
     req.add_header("User-Agent","Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/61.0.3163.100 Safari/537.36")
     resp = request.urlopen(req)
     print(resp.read().decode('utf-8'))
     
### Post带参数
    req = Request('http://www.thsrc.com.tw/tw/TimeTable/SearchResult')
    postData = parse.urlencode([
        ('StartStation', '977abb69-413a-4ccf-a109-0272c24fd490'),
        ('EndStation', '2f940836-cedc-41ef-8e28-c2336ac8fe68'),
        ('SearchDate', '2017/10/06'),
        ('SearchTime', '17:00'),
        ('SearchWay', 'DepartureInMandarin')
    ])
    req.add_header('Origin','http://www.thsrc.com.tw')
    req.add_header("User-Agent","Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/61.0.3163.100 Safari/537.36")
    resp = request.urlopen(req,data=postData.encode('utf-8'))
    print(resp.read().decode('utf-8'))
    
## BeautifulSoup-解析html
>文档地址  [中文文档](https://www.crummy.com/software/BeautifulSoup/bs4/doc/index.zh.html)
>

code
```
html_doc = """
<html><head><title>The Dormouse's story</title></head>
<body>
<p class="title"><b>The Dormouse's story</b></p>
<p class="story">
    Once upon a time there were three little sisters; and their names were
    <a href="http://example.com/elsie" class="sister" id="link1">Elsie</a>,
    <a href="http://example.com/lacie" class="sister" id="link2">Lacie</a> and
    <a href="http://example.com/tillie" class="sister1" id="link3">Tillie</a>;
    and they lived at the bottom of a well.
</p>

<p class="story">...</p>
"""

soup = bs4.BeautifulSoup(html_doc, "html.parser")
print(soup.a) #打印<a class="sister" href="http://example.com/elsie" id="link1">Elsie</a>
print(soup.a.string) #打印a标签的内容
print(soup.a['href']) #打印a标签的href属性的值
print(soup.find(id='link3'))
print(soup.find('a',class_='sister'))
print(soup.find_all('a',class_='sister'))
print(soup.find('p',{'class','story'}).get_text())
print(soup.find_all("a",href=re.compile(r'^http://example.com')))
print(soup.find_all("input",type=re.compile('text'))) 
```

## pdfminer3k-解析pdf
```
import logging
from urllib.request import urlopen

logging.Logger.propagate = False
logging.getLogger().setLevel(logging.ERROR)

from pdfminer.converter import PDFPageAggregator
from pdfminer.layout import LAParams
from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter
from pdfminer.pdfparser import PDFParser, PDFDocument

fp = open('template/pdftest.pdf', 'rb')
# 在线
# fp = urlopen('http://www.tencent.com/zh-cn/articles/8003251479983154.pdf')

# 创建一个与文档关联的解析器
parser = PDFParser(fp)

# PDF文档对象
doc = PDFDocument()

# 链接解析器和文档对象
parser.set_document(doc)
doc.set_parser(parser)

# 初始化文档
doc.initialize("")

# 创建DPF资源管理器
resource = PDFResourceManager()

# 参数分析器
laparam = LAParams()

# 聚合器
device = PDFPageAggregator(resource, laparams=laparam)

# 创建页面解析器
interpreter = PDFPageInterpreter(resource, device)

# 使用文档对象从pdf中读取内容
for page in doc.get_pages():
    # 使用页面解析器
    interpreter.process_page(page)

    layout = device.get_result()
    # 使用聚合器获取内容
    for out in layout:
        # 判断是否有get_text属性
        if hasattr(out, 'get_text'):
            print(out.get_text())

if __name__ == '__main__':
    pass
```

